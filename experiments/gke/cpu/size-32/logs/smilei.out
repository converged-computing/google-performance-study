Defaulted container "smilei" out of: smilei, flux-view (init)
chmod: cannot access './wait-fs': No such file or directory
mv: cannot stat './wait-fs': No such file or directory
#!/bin/bash
set -euo pipefail
mkdir -p /tmp/output
flux resource list
for i in {1..3}
do
  echo "FLUX-RUN START smilei-iter-$i"
  flux run --setattr=user.study_id=smilei-iter-$i -N32 -n 2048 -o cpu-affinity=per-task -o gpu-affinity=off     smilei benchmarks/tst2d_v_o2_multiphoton_Breit_Wheeler.py "Main.number_of_patches=[32,64]" "Main.simulation_time=10000" |& tee /tmp/smilei.out
  
   echo "FLUX-RUN END smilei-iter-$i"
done


output=./results/${app}
(apt-get update && apt-get install -y jq) || (yum update -y && yum install -y jq)
mkdir -p $output
for jobid in $(flux jobs -a --json | jq -r .jobs[].id); do
    echo
    study_id=$(flux job info $jobid jobspec | jq -r ".attributes.user.study_id")
    echo "FLUX-JOB START ${jobid} ${study_id}"
    echo "FLUX-JOB-JOBSPEC START"
    flux job info $jobid jobspec
    echo "FLUX-JOB-JOBSPEC END" 
    
    echo "FLUX-JOB-RESOURCES START"
    flux job info ${jobid} R
    echo "FLUX-JOB-RESOURCES END"
    echo "FLUX-JOB-EVENTLOG START" 
    flux job info $jobid guest.exec.eventlog
    echo "FLUX-JOB-EVENTLOG END" 
    echo "FLUX-JOB END ${jobid} ${study_id}"
done
echo "FLUX JOB STATS"
flux job stats         

     STATE NNODES   NCORES    NGPUS NODELIST
      free     32     2816        0 smilei-[0-31]
 allocated      0        0        0 
      down      0        0        0 
FLUX-RUN START smilei-iter-1
                    _            _
  ___           _  | |        _  \ \   Version : 5.1-231-gd06ccf95d-master
 / __|  _ __   (_) | |  ___  (_)  | |   
 \__ \ | '  \   _  | | / -_)  _   | |
 |___/ |_|_|_| |_| |_| \___| |_|  | |  
                                 /_/    
 
 

 Reading the simulation parameters
 -------------------------------------------------------------------------------
 HDF5 version 1.10.10
 Python version 3.12.3
[;33m
[WARNING](0) src/Params/Params.cpp:111 (Params) Numpy not found. Some options will not be available[0m
	 Parsing pyinit.py
	 Parsing 5.1-231-gd06ccf95d-master
	 Parsing pyprofiles.py
	 Parsing benchmarks/tst2d_v_o2_multiphoton_Breit_Wheeler.py
	 Parsing Main.number_of_patches=[32,64]
	 Parsing Main.simulation_time=10000
	 Parsing pycontrol.py
	 Check for function preprocess()
	 python preprocess function does not exist
	 Calling python _smilei_check
	 Calling python _prepare_checkpoint_dir
	 Calling python _keep_python_running() :
[1;36mCAREFUL: Patches distribution: hilbertian
[0m
	 Smilei will run on CPU devices
[;33m
[WARNING](0) src/Params/Params.cpp:1160 (compute) simulation_time has been redefined from 10000.000000 to 9999.967208 to match timestep.[0m
[1;31m --------------------------------------------------------------------------------
 [ERROR](0) src/Params/Params.cpp:1215 (compute) 
 A probem was found in the namelist:
 > ERROR in dimension 1. Patches length = 4 cells must be at least 6 cells long. Increase number of cells or reduce number of patches in this direction. 

 Find out more: https://smileipic.github.io/Smilei/namelist.html#main-variables
 --------------------------------------------------------------------------------[0m
Stack trace (most recent call last):
#8    Object "[0xffffffffffffffff]", at 0xffffffffffffffff, in 
#7    Object "smilei", at 0x56598e7c31c4, in _start
#6    Object "/lib/x86_64-linux-gnu/libc.so.6", at 0x7efaf222428a, in __libc_start_main
#5    Object "/lib/x86_64-linux-gnu/libc.so.6", at 0x7efaf22241c9, in 
#4    Object "smilei", at 0x56598e7c0185, in main
#3    Object "smilei", at 0x56598eb59f1d, in Params::Params(SmileiMPI*, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >)
#2    Object "smilei", at 0x56598eb4bb70, in Params::compute()
#1    Object "/lib/x86_64-linux-gnu/libc.so.6", at 0x7efaf223f27d, in gsignal
#0    Object "/lib/x86_64-linux-gnu/libc.so.6", at 0x7efaf2298b2c, in pthread_kill
Aborted (Signal sent by tkill() 103 0)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
--------------------------------------------------------------------------
usually indicates a failure in the peer process (e.g., a crash or
An MPI communication peer process has unexpectedly disconnected.  This
otherwise exiting without calling MPI_FINALIZE first).
usually indicates a failure in the peer process (e.g., a crash or

otherwise exiting without calling MPI_FINALIZE first).
Although this local MPI process will likely now behave unpredictably

(it may even hang or crash), the root cause of this problem is the
Although this local MPI process will likely now behave unpredictably
--------------------------------------------------------------------------
failure of the peer -- that is what you need to investigate.  For
(it may even hang or crash), the root cause of this problem is the
An MPI communication peer process has unexpectedly disconnected.  This
example, there may be a core file that you can examine.  More
failure of the peer -- that is what you need to investigate.  For
usually indicates a failure in the peer process (e.g., a crash or
generally: such peer hangups are frequently caused by application bugs
example, there may be a core file that you can examine.  More
otherwise exiting without calling MPI_FINALIZE first).
--------------------------------------------------------------------------
or other external events.
generally: such peer hangups are frequently caused by application bugs

An MPI communication peer process has unexpectedly disconnected.  This

or other external events.
Although this local MPI process will likely now behave unpredictably
usually indicates a failure in the peer process (e.g., a crash or
  Local host: smilei-2

(it may even hang or crash), the root cause of this problem is the
otherwise exiting without calling MPI_FINALIZE first).
  Local PID:  69
  Local host: smilei-16
failure of the peer -- that is what you need to investigate.  For

  Peer host:  smilei-0
  Local PID:  69
example, there may be a core file that you can examine.  More
Although this local MPI process will likely now behave unpredictably
--------------------------------------------------------------------------
  Peer host:  smilei-0
generally: such peer hangups are frequently caused by application bugs
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
--------------------------------------------------------------------------
or other external events.
example, there may be a core file that you can examine.  More

generally: such peer hangups are frequently caused by application bugs
  Local host: smilei-4
or other external events.
  Local PID:  69

  Peer host:  smilei-0
  Local host: smilei-8
--------------------------------------------------------------------------
  Local PID:  69
  Peer host:  smilei-0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: smilei-1
  Local PID:  69
  Peer host:  smilei-0
--------------------------------------------------------------------------
34.242s: flux-shell[0]: FATAL: doom: rank 0 exited and exit-timeout=30s has expired
34.245s: job.exception type=exec severity=0 rank 0 exited and exit-timeout=30s has expired
flux-job: task(s) exited with exit code 143
