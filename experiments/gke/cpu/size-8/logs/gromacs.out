Defaulted container "gromacs" out of: gromacs, flux-view (init)
#!/bin/bash
set -euo pipefail
mkdir -p /tmp/output
flux resource list
for i in {1..5}
do
  echo "FLUX-RUN START gromacs-iter-$i"
  flux run --setattr=user.study_id=gromacs-iter-$i -N8 -n 704 -o cpu-affinity=per-task -o gpu-affinity=off     gmx_mpi mdrun -v -deffnm system -s reference_s.tpr -ntomp 1 |& tee /tmp/gromacs.out
  
   echo "FLUX-RUN END gromacs-iter-$i"
done


output=./results/${app}
(apt-get update && apt-get install -y jq) || (yum update -y && yum install -y jq)
mkdir -p $output
for jobid in $(flux jobs -a --json | jq -r .jobs[].id); do
    echo
    study_id=$(flux job info $jobid jobspec | jq -r ".attributes.user.study_id")
    echo "FLUX-JOB START ${jobid} ${study_id}"
    echo "FLUX-JOB-JOBSPEC START"
    flux job info $jobid jobspec
    echo "FLUX-JOB-JOBSPEC END" 
    
    echo "FLUX-JOB-RESOURCES START"
    flux job info ${jobid} R
    echo "FLUX-JOB-RESOURCES END"
    echo "FLUX-JOB-EVENTLOG START" 
    flux job info $jobid guest.exec.eventlog
    echo "FLUX-JOB-EVENTLOG END" 
    echo "FLUX-JOB END ${jobid} ${study_id}"
done
echo "FLUX JOB STATS"
flux job stats         

     STATE NNODES   NCORES    NGPUS NODELIST
      free      8      704        0 gromacs-[0-7]
 allocated      0        0        0 
      down      0        0        0 
FLUX-RUN START gromacs-iter-1
                      :-) GROMACS - gmx mdrun, 2024.2 (-:

Executable:   /usr/bin/gmx_mpi
Data prefix:  /usr
Working dir:  /opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon
Command line:
  gmx_mpi mdrun -v -deffnm system -s reference_s.tpr -ntomp 1

Reading file reference_s.tpr, VERSION 2019-dev-20180518-7d5382b-local (single precision)
Note: file tpx version 113, software tpx version 133
Using 704 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process

starting mdrun 'Argon'
20 steps,      0.0 ps.
step 0
vol 0.83  imb F 1155% 
Writing final coordinates.
step 20, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 391.8%.
 The balanceable part of the MD step is 2%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 6.7%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %

NOTE: 6.7 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You can consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.

NOTE: 33 % of the run time was spent in domain decomposition,
      0 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 62 % of the run time was spent communicating energies,
      you might want to increase some nst* mdp options

               Core t (s)   Wall t (s)        (%)
       Time:       71.257        0.103    69383.5
                 (ns/day)    (hour/ns)
Performance:       35.334        0.679

GROMACS reminds you: "It's Time to Move On" (F. Black)

FLUX-RUN END gromacs-iter-1
FLUX-RUN START gromacs-iter-2
                      :-) GROMACS - gmx mdrun, 2024.2 (-:

Executable:   /usr/bin/gmx_mpi
Data prefix:  /usr
Working dir:  /opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon
Command line:
  gmx_mpi mdrun -v -deffnm system -s reference_s.tpr -ntomp 1


Back Off! I just backed up system.log to ./#system.log.1#
Reading file reference_s.tpr, VERSION 2019-dev-20180518-7d5382b-local (single precision)
Note: file tpx version 113, software tpx version 133
Using 704 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Back Off! I just backed up system.trr to ./#system.trr.1#

Back Off! I just backed up system.edr to ./#system.edr.1#
starting mdrun 'Argon'
20 steps,      0.0 ps.
step 0
vol 0.82  imb F 530% 
Writing final coordinates.

Back Off! I just backed up system.gro to ./#system.gro.1#
step 20, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 328.4%.
 The balanceable part of the MD step is 2%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 6.5%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %

NOTE: 6.5 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You can consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.

NOTE: 51 % of the run time was spent in domain decomposition,
      1 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 42 % of the run time was spent communicating energies,
      you might want to increase some nst* mdp options

               Core t (s)   Wall t (s)        (%)
       Time:       50.277        0.072    70110.8
                 (ns/day)    (hour/ns)
Performance:       50.603        0.474

GROMACS reminds you: "I Snipe Like Wesley" (Urban Dance Squad)

FLUX-RUN END gromacs-iter-2
FLUX-RUN START gromacs-iter-3
                      :-) GROMACS - gmx mdrun, 2024.2 (-:

Executable:   /usr/bin/gmx_mpi
Data prefix:  /usr
Working dir:  /opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon
Command line:
  gmx_mpi mdrun -v -deffnm system -s reference_s.tpr -ntomp 1


Back Off! I just backed up system.log to ./#system.log.2#
Reading file reference_s.tpr, VERSION 2019-dev-20180518-7d5382b-local (single precision)
Note: file tpx version 113, software tpx version 133
Using 704 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Back Off! I just backed up system.trr to ./#system.trr.2#

Back Off! I just backed up system.edr to ./#system.edr.2#
starting mdrun 'Argon'
20 steps,      0.0 ps.
step 0
vol 0.86  imb F 388% 
Writing final coordinates.

Back Off! I just backed up system.gro to ./#system.gro.2#
step 20, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 352.4%.
 The balanceable part of the MD step is 2%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 6.4%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %

NOTE: 6.4 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You can consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.

NOTE: 31 % of the run time was spent in domain decomposition,
      0 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 64 % of the run time was spent communicating energies,
      you might want to increase some nst* mdp options

               Core t (s)   Wall t (s)        (%)
       Time:       79.459        0.113    70122.2
                 (ns/day)    (hour/ns)
Performance:       32.024        0.749

GROMACS reminds you: "There is no such thing as free energy. Anyone who advocates it does not know what he is talking about." (Alireza Haghighat)

FLUX-RUN END gromacs-iter-3
FLUX-RUN START gromacs-iter-4
                      :-) GROMACS - gmx mdrun, 2024.2 (-:

Executable:   /usr/bin/gmx_mpi
Data prefix:  /usr
Working dir:  /opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon
Command line:
  gmx_mpi mdrun -v -deffnm system -s reference_s.tpr -ntomp 1


Back Off! I just backed up system.log to ./#system.log.3#
Reading file reference_s.tpr, VERSION 2019-dev-20180518-7d5382b-local (single precision)
Note: file tpx version 113, software tpx version 133
Using 704 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Back Off! I just backed up system.trr to ./#system.trr.3#

Back Off! I just backed up system.edr to ./#system.edr.3#
starting mdrun 'Argon'
20 steps,      0.0 ps.
step 0
vol 0.87  imb F 556% 
Writing final coordinates.

Back Off! I just backed up system.gro to ./#system.gro.3#
step 20, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 349.5%.
 The balanceable part of the MD step is 2%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 6.3%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %

NOTE: 6.3 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You can consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.

NOTE: 38 % of the run time was spent in domain decomposition,
      0 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 58 % of the run time was spent communicating energies,
      you might want to increase some nst* mdp options

               Core t (s)   Wall t (s)        (%)
       Time:       78.331        0.115    68184.2
                 (ns/day)    (hour/ns)
Performance:       31.587        0.760

GROMACS reminds you: "You Fill Your Space So Sweet" (F. Apple)

FLUX-RUN END gromacs-iter-4
FLUX-RUN START gromacs-iter-5
                      :-) GROMACS - gmx mdrun, 2024.2 (-:

Executable:   /usr/bin/gmx_mpi
Data prefix:  /usr
Working dir:  /opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon
Command line:
  gmx_mpi mdrun -v -deffnm system -s reference_s.tpr -ntomp 1


Back Off! I just backed up system.log to ./#system.log.4#
Reading file reference_s.tpr, VERSION 2019-dev-20180518-7d5382b-local (single precision)
Note: file tpx version 113, software tpx version 133
Using 704 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Back Off! I just backed up system.trr to ./#system.trr.4#

Back Off! I just backed up system.edr to ./#system.edr.4#
starting mdrun 'Argon'
20 steps,      0.0 ps.
step 0
vol 0.82  imb F 510% 
Writing final coordinates.

Back Off! I just backed up system.gro to ./#system.gro.4#
step 20, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 378.2%.
 The balanceable part of the MD step is 1%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 4.2%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %


NOTE: 36 % of the run time was spent in domain decomposition,
      0 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 59 % of the run time was spent communicating energies,
      you might want to increase some nst* mdp options

               Core t (s)   Wall t (s)        (%)
       Time:       80.751        0.115    70101.9
                 (ns/day)    (hour/ns)
Performance:       31.503        0.762

GROMACS reminds you: "But I always say, one's company, two's a crowd, and three's a party." (Andy Warhol)

FLUX-RUN END gromacs-iter-5
0% [Working]            Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease
0% [Connecting to security.ubuntu.com (185.125.190.82)]                                                       Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]
0% [2 InRelease 6932 B/128 kB 5%] [Connecting to security.ubuntu.com (185.125.1                                                                               0% [2 InRelease 59.1 kB/128 kB 46%] [Waiting for headers]                                                         Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]
0% [2 InRelease 117 kB/128 kB 91%] [3 InRelease 14.2 kB/129 kB 11%]                                                                   0% [3 InRelease 14.2 kB/129 kB 11%]                                   Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]
0% [4 InRelease 2588 B/127 kB 2%] [3 InRelease 14.2 kB/129 kB 11%]0% [4 InRelease 110 kB/127 kB 86%] [3 InRelease 14.2 kB/129 kB 11%]                                                                   0% [3 InRelease 14.2 kB/129 kB 11%]                                   Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1544 kB]
0% [5 Packages 5759 B/1544 kB 0%] [3 InRelease 17.1 kB/129 kB 13%]0% [5 Packages 240 kB/1544 kB 16%] [3 InRelease 31.5 kB/129 kB 24%]                                                                   0% [Waiting for headers] [3 InRelease 48.9 kB/129 kB 38%]                                                         Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3200 kB]
0% [6 Packages 241 kB/3200 kB 8%] [3 InRelease 48.9 kB/129 kB 38%]0% [5 Packages store 0 B] [6 Packages 241 kB/3200 kB 8%] [3 InRelease 48.9 kB/10% [5 Packages store 0 B] [Waiting for headers] [3 InRelease 69.2 kB/129 kB 54%                                                                               Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]
0% [5 Packages store 0 B] [7 Packages 13.3 kB/55.7 kB 24%] [3 InRelease 69.2 kB0% [5 Packages store 0 B] [Waiting for headers] [3 InRelease 69.2 kB/129 kB 54%                                                                               Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4420 kB]
0% [5 Packages store 0 B] [8 Packages 22.9 kB/4420 kB 1%] [3 InRelease 69.2 kB/                                                                               0% [8 Packages 2093 kB/4420 kB 47%] [3 InRelease 77.9 kB/129 kB 60%]0% [6 Packages store 0 B] [8 Packages 2093 kB/4420 kB 47%] [3 InRelease 77.9 kB                                                                               0% [6 Packages store 0 B] [3 InRelease 77.9 kB/129 kB 60%]                                                          0% [3 InRelease 113 kB/129 kB 87%]0% [7 Packages store 0 B] [3 InRelease 113 kB/129 kB 87%]                                                         0% [3 InRelease 113 kB/129 kB 87%]0% [8 Packages store 0 B] [3 InRelease 113 kB/129 kB 87%]                                                         0% [8 Packages store 0 B]90% [8 Packages store 0 B]                          90% [Waiting for headers]                         Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1244 kB]
90% [9 Packages 2655 B/1244 kB 0%]                                  99% [Working]99% [9 Packages store 0 B]                          100% [Working]              Fetched 10.8 MB in 1s (8057 kB/s)
Reading package lists... 0%Reading package lists... 0%Reading package lists... 0%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 46%Reading package lists... 46%Reading package lists... 47%Reading package lists... 47%Reading package lists... 56%Reading package lists... 56%Reading package lists... 69%Reading package lists... 69%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 83%Reading package lists... 83%Reading package lists... 95%Reading package lists... 95%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
Reading package lists... 0%Reading package lists... 0%Reading package lists... 0%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 46%Reading package lists... 46%Reading package lists... 47%Reading package lists... 47%Reading package lists... 56%Reading package lists... 56%Reading package lists... 69%Reading package lists... 69%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 74%Reading package lists... 83%Reading package lists... 83%Reading package lists... 95%Reading package lists... 95%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 50%Building dependency tree... 50%Building dependency tree... Done
Reading state information... 0% Reading state information... 0%Reading state information... Done
jq is already the newest version (1.6-2.1ubuntu3).
0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.

FLUX-JOB START 337691803648 gromacs-iter-5
FLUX-JOB-JOBSPEC START
{"resources": [{"type": "node", "count": 8, "with": [{"type": "slot", "count": 88, "with": [{"type": "core", "count": 1}], "label": "task"}]}], "tasks": [{"command": ["gmx_mpi", "mdrun", "-v", "-deffnm", "system", "-s", "reference_s.tpr", "-ntomp", "1"], "slot": "task", "count": {"per_slot": 1}}], "attributes": {"system": {"duration": 0, "cwd": "/opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon", "shell": {"options": {"rlimit": {"cpu": -1, "fsize": -1, "data": -1, "stack": 8388608, "core": -1, "nofile": 1048576, "as": -1, "rss": -1, "nproc": -1}, "cpu-affinity": "per-task", "gpu-affinity": "off"}}}, "user": {"study_id": "gromacs-iter-5"}}, "version": 1}
FLUX-JOB-JOBSPEC END
FLUX-JOB-RESOURCES START
{"version": 1, "execution": {"R_lite": [{"rank": "0-7", "children": {"core": "0-87"}}], "nodelist": ["gromacs-[0-7]"], "starttime": 1746401180, "expiration": 4900001160}}
FLUX-JOB-RESOURCES END
FLUX-JOB-EVENTLOG START
{"timestamp":1746401180.8321807,"name":"init"}
{"timestamp":1746401180.8327241,"name":"starting"}
{"timestamp":1746401180.8449147,"name":"shell.init","context":{"service":"0-shell-f9sVfmDh","leader-rank":0,"size":8}}
{"timestamp":1746401181.057653,"name":"shell.start","context":{"taskmap":{"version":1,"map":[[0,8,88,1]]}}}
{"timestamp":1746401185.3793852,"name":"shell.task-exit","context":{"localid":82,"rank":82,"state":"Exited","pid":978,"wait_status":0,"signaled":0,"exitcode":0}}
{"timestamp":1746401185.4193022,"name":"complete","context":{"status":0}}
{"timestamp":1746401185.4193244,"name":"done"}

FLUX-JOB-EVENTLOG END
FLUX-JOB END 337691803648 gromacs-iter-5

FLUX-JOB START 259677749248 gromacs-iter-4
FLUX-JOB-JOBSPEC START
{"resources": [{"type": "node", "count": 8, "with": [{"type": "slot", "count": 88, "with": [{"type": "core", "count": 1}], "label": "task"}]}], "tasks": [{"command": ["gmx_mpi", "mdrun", "-v", "-deffnm", "system", "-s", "reference_s.tpr", "-ntomp", "1"], "slot": "task", "count": {"per_slot": 1}}], "attributes": {"system": {"duration": 0, "cwd": "/opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon", "shell": {"options": {"rlimit": {"cpu": -1, "fsize": -1, "data": -1, "stack": 8388608, "core": -1, "nofile": 1048576, "as": -1, "rss": -1, "nproc": -1}, "cpu-affinity": "per-task", "gpu-affinity": "off"}}}, "user": {"study_id": "gromacs-iter-4"}}, "version": 1}
FLUX-JOB-JOBSPEC END
FLUX-JOB-RESOURCES START
{"version": 1, "execution": {"R_lite": [{"rank": "0-7", "children": {"core": "0-87"}}], "nodelist": ["gromacs-[0-7]"], "starttime": 1746401176, "expiration": 4900001160}}
FLUX-JOB-RESOURCES END
FLUX-JOB-EVENTLOG START
{"timestamp":1746401176.1820393,"name":"init"}
{"timestamp":1746401176.182595,"name":"starting"}
{"timestamp":1746401176.1944973,"name":"shell.init","context":{"service":"0-shell-f7pdqLpF","leader-rank":0,"size":8}}
{"timestamp":1746401176.4074683,"name":"shell.start","context":{"taskmap":{"version":1,"map":[[0,8,88,1]]}}}
{"timestamp":1746401180.6322391,"name":"shell.task-exit","context":{"localid":85,"rank":437,"state":"Exited","pid":763,"wait_status":0,"signaled":0,"exitcode":0}}
{"timestamp":1746401180.6942992,"name":"complete","context":{"status":0}}
{"timestamp":1746401180.6943185,"name":"done"}

FLUX-JOB-EVENTLOG END
FLUX-JOB END 259677749248 gromacs-iter-4

FLUX-JOB START 181613363200 gromacs-iter-3
FLUX-JOB-JOBSPEC START
{"resources": [{"type": "node", "count": 8, "with": [{"type": "slot", "count": 88, "with": [{"type": "core", "count": 1}], "label": "task"}]}], "tasks": [{"command": ["gmx_mpi", "mdrun", "-v", "-deffnm", "system", "-s", "reference_s.tpr", "-ntomp", "1"], "slot": "task", "count": {"per_slot": 1}}], "attributes": {"system": {"duration": 0, "cwd": "/opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon", "shell": {"options": {"rlimit": {"cpu": -1, "fsize": -1, "data": -1, "stack": 8388608, "core": -1, "nofile": 1048576, "as": -1, "rss": -1, "nproc": -1}, "cpu-affinity": "per-task", "gpu-affinity": "off"}}}, "user": {"study_id": "gromacs-iter-3"}}, "version": 1}
FLUX-JOB-JOBSPEC END
FLUX-JOB-RESOURCES START
{"version": 1, "execution": {"R_lite": [{"rank": "0-7", "children": {"core": "0-87"}}], "nodelist": ["gromacs-[0-7]"], "starttime": 1746401171, "expiration": 4900001160}}
FLUX-JOB-RESOURCES END
FLUX-JOB-EVENTLOG START
{"timestamp":1746401171.5285561,"name":"init"}
{"timestamp":1746401171.5291488,"name":"starting"}
{"timestamp":1746401171.5412943,"name":"shell.init","context":{"service":"0-shell-f5mhYxZm","leader-rank":0,"size":8}}
{"timestamp":1746401171.7519636,"name":"shell.start","context":{"taskmap":{"version":1,"map":[[0,8,88,1]]}}}
{"timestamp":1746401175.9808755,"name":"shell.task-exit","context":{"localid":84,"rank":612,"state":"Exited","pid":584,"wait_status":0,"signaled":0,"exitcode":0}}
{"timestamp":1746401176.0434501,"name":"complete","context":{"status":0}}
{"timestamp":1746401176.0434756,"name":"done"}

FLUX-JOB-EVENTLOG END
FLUX-JOB END 181613363200 gromacs-iter-3

FLUX-JOB START 104371060736 gromacs-iter-2
FLUX-JOB-JOBSPEC START
{"resources": [{"type": "node", "count": 8, "with": [{"type": "slot", "count": 88, "with": [{"type": "core", "count": 1}], "label": "task"}]}], "tasks": [{"command": ["gmx_mpi", "mdrun", "-v", "-deffnm", "system", "-s", "reference_s.tpr", "-ntomp", "1"], "slot": "task", "count": {"per_slot": 1}}], "attributes": {"system": {"duration": 0, "cwd": "/opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon", "shell": {"options": {"rlimit": {"cpu": -1, "fsize": -1, "data": -1, "stack": 8388608, "core": -1, "nofile": 1048576, "as": -1, "rss": -1, "nproc": -1}, "cpu-affinity": "per-task", "gpu-affinity": "off"}}}, "user": {"study_id": "gromacs-iter-2"}}, "version": 1}
FLUX-JOB-JOBSPEC END
FLUX-JOB-RESOURCES START
{"version": 1, "execution": {"R_lite": [{"rank": "0-7", "children": {"core": "0-87"}}], "nodelist": ["gromacs-[0-7]"], "starttime": 1746401166, "expiration": 4900001160}}
FLUX-JOB-RESOURCES END
FLUX-JOB-EVENTLOG START
{"timestamp":1746401166.9251943,"name":"init"}
{"timestamp":1746401166.925705,"name":"starting"}
{"timestamp":1746401166.9372435,"name":"shell.init","context":{"service":"0-shell-f3k1uy8B","leader-rank":0,"size":8}}
{"timestamp":1746401167.1458373,"name":"shell.start","context":{"taskmap":{"version":1,"map":[[0,8,88,1]]}}}
{"timestamp":1746401171.3245733,"name":"shell.task-exit","context":{"localid":85,"rank":85,"state":"Exited","pid":447,"wait_status":0,"signaled":0,"exitcode":0}}
{"timestamp":1746401171.3876874,"name":"complete","context":{"status":0}}
{"timestamp":1746401171.3877144,"name":"done"}

FLUX-JOB-EVENTLOG END
FLUX-JOB END 104371060736 gromacs-iter-2

FLUX-JOB START 24326963200 gromacs-iter-1
FLUX-JOB-JOBSPEC START
{"resources": [{"type": "node", "count": 8, "with": [{"type": "slot", "count": 88, "with": [{"type": "core", "count": 1}], "label": "task"}]}], "tasks": [{"command": ["gmx_mpi", "mdrun", "-v", "-deffnm", "system", "-s", "reference_s.tpr", "-ntomp", "1"], "slot": "task", "count": {"per_slot": 1}}], "attributes": {"system": {"duration": 0, "cwd": "/opt/gromacs-2024.2/build/tests/regressiontests-2024.2/complex/argon", "shell": {"options": {"rlimit": {"cpu": -1, "fsize": -1, "data": -1, "stack": 8388608, "core": -1, "nofile": 1048576, "as": -1, "rss": -1, "nproc": -1}, "cpu-affinity": "per-task", "gpu-affinity": "off"}}}, "user": {"study_id": "gromacs-iter-1"}}, "version": 1}
FLUX-JOB-JOBSPEC END
FLUX-JOB-RESOURCES START
{"version": 1, "execution": {"R_lite": [{"rank": "0-7", "children": {"core": "0-87"}}], "nodelist": ["gromacs-[0-7]"], "starttime": 1746401162, "expiration": 4900001160}}
FLUX-JOB-RESOURCES END
FLUX-JOB-EVENTLOG START
{"timestamp":1746401162.1540873,"name":"init"}
{"timestamp":1746401162.1546142,"name":"starting"}
{"timestamp":1746401162.1662295,"name":"shell.init","context":{"service":"0-shell-fe4h3d1","leader-rank":0,"size":8}}
{"timestamp":1746401162.3814969,"name":"shell.start","context":{"taskmap":{"version":1,"map":[[0,8,88,1]]}}}
{"timestamp":1746401166.7283161,"name":"shell.task-exit","context":{"localid":51,"rank":227,"state":"Exited","pid":164,"wait_status":0,"signaled":0,"exitcode":0}}
{"timestamp":1746401166.7942183,"name":"complete","context":{"status":0}}
{"timestamp":1746401166.7942512,"name":"done"}

FLUX-JOB-EVENTLOG END
FLUX-JOB END 24326963200 gromacs-iter-1
FLUX JOB STATS
{"job_states":{"depend":0,"priority":0,"sched":0,"run":0,"cleanup":0,"inactive":5,"total":5},"successful":5,"failed":0,"canceled":0,"timeout":0,"inactive_purged":0,"queues":[]}
